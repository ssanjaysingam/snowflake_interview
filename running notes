>> Cluster Key
Use the system function, SYSTEM$CLUSTERING_INFORMATION, to calculate clustering details, including clustering depth, for a given table.

Clustering key consists of one or more table columns/expressions, which can be of any data type, except GEOGRAPHY, VARIANT, OBJECT, or ARRAY. 
A clustering key can contain any of the following:
Base columns.
Expressions on base columns.
Expressions on paths in VARIANT columns.

An existing cluster key is copied when a table is created using clone but not copied when table created using CTAS or like command.

To suspend/resuming Automatic Clustering for a table:
alter table t1 suspend recluster;
alter table t1 resume recluster;

Viewing automatic clustering history:
AUTOMATIC_CLUSTERING_HISTORY
Snowsight: Admin > Usage

For most tables, Snowflake recommends a maximum of 3 or 4 columns (or expressions) per key. Adding more than 3-4 columns tends to increase costs more than benefits.

If you are defining a multi-column clustering key for a table, the order in which the columns are specified in the CLUSTER BY clause is important. As a general rule, 
Snowflake recommends ordering the columns from lowest cardinality to highest cardinality.
When clustering on a text field, the cluster key metadata only tracks the first 6 characters. For string fields that need more characters to get unique values, 
cluster on the HASH of the column instead of the column itself.
For example, if all values in the event_type column begin with event_, you should cluster by HASH(event_type)

>> Show Functions:
Lists all the native (i.e. system-defined/built-in) scalar functions provided by Snowflake, 
as well as any user-defined functions (UDFs) or external functions that have been created for your account.

The command does not require a running warehouse to execute.
The command returns a maximum of 10K records for the specified object type, as dictated by the access privileges 
for the role used to execute the command; any records above the 10K limit are not returned, even with a filter applied.
To view results for which more than 10K records exist, query the corresponding view (if one exists) in the Snowflake Information Schema.

>> Constraints:
Snowflake supports the following constraint types from the ANSI SQL standard:
UNIQUE, PRIMARY KEY, FOREIGN KEY, NOT NULL

Snowflake supports defining and maintaining constraints, but does not enforce them, except for NOT NULL constraints, which are always enforced.

>> Stages
To stage files to a table stage, list the files, query them on the stage, or drop them, you must be the 
table owner (have the role with the OWNERSHIP privilege on the table).

>>INFER_SCHEMA
Automatically detects the file metadata schema in a set of staged data files that contain semi-structured data and retrieves the column definitions.
This feature is currently limited to Apache Parquet, Apache Avro, and ORC files.

>> GENERATE_COLUMN_DESCRIPTION
Generates a list of columns from a set of staged files that contain semi-structured data using the INFER_SCHEMA function output.
-- Create a file format that sets the file type as Parquet.
create file format my_parquet_format
  type = parquet;

-- Query the GENERATE_COLUMN_DESCRIPTION function.
select generate_column_description(array_agg(object_construct(*)), 'table') as columns
  from table (
    infer_schema(
      location=>'@mystage',
      file_format=>'my_parquet_format'
    )
  );
  
>> Loading
The wizard is only intended for loading small numbers of files of limited size (up to 50 MB).
For loading larger files or large numbers of files, we recommend using the Snowflake client, SnowSQL

>>Search Optimization Service
The search optimization service can significantly improve the performance of point lookup queries.
Search optimization is a table-level property and applies to all columns with supported data types
A maintenance service that runs in the background is responsible for creating and maintaining the search access path.
The search optimization service speeds only equality searches.

You must have OWNERSHIP privilege on the table.
You must have ADD SEARCH OPTIMIZATION privilege on the schema that contains the table.

Use APPROX_COUNT_DISTINCT to get the approximate number of distinct values.
Data Types Supported By the Search Optimization Service

The search optimization service currently supports equality predicate and IN list predicate searches for the following data types:
Fixed-point numbers (e.g. INTEGER, NUMERIC).
DATE, TIME, and TIMESTAMP.
VARCHAR.
BINARY.

ALTER TABLE [IF EXISTS] <table_name> ADD/DROP SEARCH OPTIMIZATION;


>> Shares:

If you plan to securely share data with data consumers across different regions or cloud platforms, note that currently,
replicating a primary database is blocked if one or more external tables exist in the database.

A new object created in a database in a share is not automatically available to consumers.
To make the object available to consumers, you must use the GRANT <privilege> … TO SHARE command to explicitly add the object to the share.

Do not include secure objects that use the CURRENT_USER or CURRENT_ROLE functions in their definition.

Enabling Data Consumers to Create Table Streams on Shared Tables
In order for data consumers to create streams on shared tables or secure views, you must enable change tracking 
on the shared tables or the underlying tables for a shared view.
In addition, the data retention period should be extended for the tables.

If you have a Business Critical account and are sharing data with consumer accounts
Snowflake supports sharing sensitive data with non-Business Critical accounts (disabled by default), but does not encourage doing so.
To ensure compliance with HIPAA and HITRUST requirements, Snowflake does not allow HIPAA accounts to share data with non-HIPAA accounts.
If you are using Tri-Secret Secure data protection, note that Snowflake treats data access from consumer accounts as if the access occurred
from within your own account.

SHOW GRANTS OF SHARE lists all accounts that have created a database from the share. 
If no accounts have created a database from the share, the results are empty.

--
Features that rely on Snowflake-managed compute resources appear on your bill as separate line items. 
Charges for these features are calculated based on total usage of the resources (including cloud service usage) 
measured in compute-hours credit usage. One compute-hour is comparable to the computing resources utilized when 
running an X-Small virtual warehouse for an hour.

The credits used by these serverless features are not monitored or controlled by resource monitors; resource monitors 
only work with the virtual warehouses that you create/manage in your account.


--Understanding local and remote disk spilling and how to mitigate the performance impact due to it.
What is disk spilling? When Snowflake warehouse cannot fit an operation in memory, it starts spilling (storing) data 
first to the local disk of a warehouse node, and then to remote storage and if need to remote cloud storage as well,
this means extra IO operations, any query that requires spilling will take longer than a similar query running on 
similar data that is capable to fit the operations in memory.

Solution:
The spilling can't always be avoided, especially for large batches of data, but it can be decreased by:
Reviewing the query for query optimization especially if it is a new query
Reducing the amount of data processed. For example, by trying to improve partition pruning, or projecting only the columns that are needed in the output.
Decreasing the number of parallel queries running in the warehouse.
Trying to split the processing into several steps (for example by replacing the CTEs with temporary tables).
Using a larger warehouse - this effectively means more memory and more local disk space.

-- File encryption in Snowflake Stage
If the stage is an internal data files are automatically encrypted by the Snowflake client on the user’s local 
machine prior to being transmitted to the internal stage, in addition to being encrypted after they are loaded into the stage.

If the stage is an external stage, the user may optionally encrypt the data files using client-side encryption.
We recommend client-side encryption for data files in external stages; but if the data is not encrypted, 
Snowflake immediately encrypts the data when it is loaded into a table.

In Snowflake, all data at rest is always encrypted and encrypted with TLS(Transport Layer Security) in transit.

All Snowflake-managed keys are automatically rotated by Snowflake when they are more than 30 days old.
When active, a key is used to encrypt data and is available for usage by the customer. When retired, 
the key is used solely to decrypt data and is only available for accessing the data.
When a key is destroyed, it is not used for either encryption or decryption. Regular key rotation limits the life cycle for the keys to a limited period of time.

--Periodic Rekeying
Periodic data rekeying completes the life cycle.While key rotation ensures that a key is transferred from its active state to a retired state,
rekeying ensures that a key is transferred from its retired state to being destroyed.
If periodic rekeying is enabled, then when the retired encryption key for a table is older than one year, Snowflake automatically creates a new encryption key
and re-encrypts all data previously protected by the retired key using the new key. The new key is used to decrypt the table data going forward.

Tri-Secret Secure
Snowflake logo in black (no text) Business Critical Feature

Requires Business Critical Edition (or higher). To inquire about upgrading, please contact Snowflake Support.

--Tri-Secret Secure
Tri-Secret Secure is the combination of a Snowflake-maintained key and a customer-managed key in the cloud provider platform that hosts your Snowflake account 
to create a composite master key to protect your Snowflake data. The composite master key acts as an account master key and wraps all of the keys in the hierarchy;
however, the composite master key never encrypts raw data.

>> Periodic rekeying of encrypted data is avaiable from Enterprise Edition
>> Configure the idle session timeout for your account or a user through session policies --from Enterprise Edition

-----------
Schema detection:
This is a feature which is enabled by “INFER SCHEMA”, this property automatically determines the file metadata structure and obtains the column definitions. 

Schema evolution:
This is a table property and when it is set, it guarantees that alterations made to a table's schema won't affect the table's capacity to handle data. 
--Below is how we can enable this feature:
ALTER TABLE <table_name> SET enable_schema_evolution=true;    -- Post this command is set, we can also use show tables; command to see which all tables have this feature enabled.

